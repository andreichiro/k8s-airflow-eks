# An example of ClusterConfig object using an existing VPC:
--- 
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: h2-lakehouse-on-k8s-eks
  region: us-east-1
  version: '1.28'
  tags:
    karpenter.sh/discovery: h2-lakehouse-on-k8s-eks

iam:
  withOIDC: true
  
karpenter:
  version: 'v0.32.1' # Exact version must be provided
  createServiceAccount: true # default is false
  withSpotInterruptionQueue: true # adds all required policies and rules for supporting Spot Interruption Queue, default is false


#vpc:
#  cidr: 10.10.0.0/16
#  autoAllocateIPv6: true
#  hostnameType: resource-name
#  privateNetworking: true  
#  clusterEndpoints:
#    publicAccess: true
#    privateAccess: true
#  nat:
#    gateway: Single #   
#   

vpc:
  id: "vpc-067fbd2559b47f9e5"  # (optional, must match VPC ID used for each subnet below)
  cidr: "10.0.0.0/16"       # (optional, must match CIDR used by the given VPC)
  clusterEndpoints:
    publicAccess: true
    privateAccess: true
  nat:
    gateway: Single #   
  subnets:
    # must provide 'private' and/or 'public' subnets by availability zone as shown
    private:
      us-east-1a:
        id: "subnet-0bbcf8ab7a35bfa3e"
        cidr: "10.0.1.0/24" # (optional, must match CIDR used by the given subnet)

      us-east-1b:
        id: "subnet-071da0d011fdec13b"
        cidr: "10.0.2.0/24"  # (optional, must match CIDR used by the given subnet)

      us-east-1c:
        id: "subnet-080344293c8afad0f"
        cidr: "10.0.3.0/24"   # (optional, must match CIDR used by the given subnet)
    
    public:
      us-east-1a:
        id: "subnet-04bd70cad689f17b6"
        cidr: "10.0.101.0/24" # (optional, must match CIDR used by the given subnet)

      us-east-1b:
        id: "subnet-06d8693cfdc5432aa"
        cidr: "10.0.102.0/24"  # (optional, must match CIDR used by the given subnet)

      us-east-1c:
        id: "subnet-055ceeaa2de42c06e"
        cidr: "10.0.103.0/24"   # (optional, must match CIDR used by the given subnet)
    
managedNodeGroups:
  - name: lakehouse-on-k8s-demand
    instanceTypes: ["t3.xlarge", "m5.xlarge", "m5.2xlarge"] 
    minSize: 3
    desiredCapacity: 4
    maxSize: 6
    availabilityZones: ["us-east-1a", "us-east-1b", "us-east-1c"]
    privateNetworking: true
    volumeSize: 500
    updateConfig:
      maxUnavailable: 1 # or `maxUnavailablePercentage: 75` to specify maxUnavailable as a percentage of total nodes
    ssh:
      allow: true
      publicKeyName: andreichiro
      # new feature for restricting SSH access to certain AWS security group IDs
    labels: {role: managed_node_group}
    # Note: unmanaged nodegroups (`nodeGroups` field) use a different structure (map[string]string) to express taints
    tags:
      nodegroup-role: managed_node_group
    iam:
      withAddonPolicies:
        ebs: true
        imageBuilder: true
        efs: true
        albIngress: true
        autoScaler: true
        cloudWatch: true
        externalDNS: true
addons:
- name: vpc-cni # no version is specified so it deploys the default version
  version: latest
- name: coredns
  version: latest # auto discovers the latest available
- name: kube-proxy
  version: latest
- name: aws-ebs-csi-driver
  version: latest
- name: eks-pod-identity-agent
  version: latest
